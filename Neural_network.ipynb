{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1VDSkXs01VlK2NWUXZ4dd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jq6D1e4nNsD2"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["class Linear_layer:\n","  \"\"\" This creates a linear layer which basically contains a matrix of weights and a vector of biases. \"\"\"\n","  def __init__(self,n_inputs,n_neurons):\n","    \"\"\" This initializes the layer with a weight matrix of shape (samples,no. of neurons in the layer). Bias vector is of shape(no. of neurons,1). \"\"\"\n","    self.weights = np.random.randn(n_inputs,n_neurons)\n","    self.bias = np.zeros(n_neurons,)\n","\n","  def forward(self,inputs):\n","    \"\"\" Performs forward propagation. Inputs are multiplied with the weights and biases are added to the product. This output will be sent to an activation function. \"\"\"\n","    self.inputs = inputs\n","    self.output = np.dot(inputs,self.weights) + self.bias\n","    return self.output\n","\n","  def backward(self,dvalues):\n","    \"\"\"Performs backpropagation. It receives the gradients of the next layer and uses it to calculate the gradients with respect to its inputs, weights and bias. \"\"\"\n","    self.dinputs = np.dot(dvalues,self.weights.T)\n","    self.dweights = np.dot(self.inputs.T,dvalues)\n","    self.dbias = np.sum(dvalues,axis=0,keepdims=True)\n","    return self.dinputs,self.dweights,self.dbias\n"],"metadata":{"id":"a3u9pj8IQCly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReLU_activation:\n","  \"\"\" This creates a ReLU activation layer. \"\"\"\n","  def __init__(self):\n","    pass\n","\n","  def forward(self,inputs):\n","    \"\"\" Returns the maximum of 0 and input. 0 is returned if input is negative. \"\"\"\n","    self.inputs = inputs\n","    self.output = np.maximum(0,self.inputs)\n","    return self.output\n","\n","  def backward(self,dvalues):\n","    \"\"\" dvalues represents the gradients of the next layer. Gradients of this layer is dvalues or 0, depending on whether the ReLU output is positive or 0, respectively. \"\"\"\n","    self.dinputs = dvalues.copy()\n","    self.dinputs *= self.inputs > 0\n","    return self.dinputs\n"],"metadata":{"id":"3aAzIIY6Sg2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class sigmoid_activation:\n","  \"\"\" This creates a sigmoid activation layer. \"\"\"\n","  def __init__(self):\n","    pass\n","\n","  def forward(self,inputs):\n","    \"\"\" Returns a value between 0 and 1. \"\"\"\n","    self.inputs = inputs\n","    self.output = 1/(1+np.exp(-self.inputs))\n","    return self.output\n","\n","  def backward(self,dvalues):\n","    \"\"\" dvalues represents the gradients of the next layer. Gradients of this layer is calculated by multiplying dvalues with derivative of sigmoid function. \"\"\"\n","    self.dinputs = dvalues.copy()\n","    self.dinputs *= self.output*(1-self.output)\n","    return self.dinputs"],"metadata":{"id":"9gteeeaiS8Qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class tanh_activation:\n","  \"\"\" This creates the tanh activation function. \"\"\"\n","  def __init__(self):\n","    pass\n","\n","  def forward(self,inputs):\n","    \"\"\" Returns a value between -1 and 1. \"\"\"\n","    self.inputs = inputs\n","    self.output = np.tanh(self.inputs)\n","    return self.output\n","\n","  def backward(self,dvalues):\n","    \"\"\" dvalues represents the gradients of the next layer. Gradients of this layer is calculated by multiplying dvalues with derivative of tanh function. \"\"\"\n","    self.dinputs = dvalues.copy()\n","    self.dinputs *= (1-self.output**2)\n","    return self.dinputs"],"metadata":{"id":"G5nJud81U6P3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class softmax_activation:\n","  \"\"\" This creates a softmax activation function. It is primarily used to calculate the probabilities of the image belonging to different classes in the dataset. \"\"\"\n","  def __init__(self):\n","    pass\n","\n","  def forward(self,inputs):\n","    \"\"\" Returns probabilities of an image belonging to different classes. \"\"\"\n","    self.inputs = inputs\n","    \"\"\" Clipping is done below to prevent numerical overflow. \"\"\"\n","    exp_values = np.exp(self.inputs - np.max(self.inputs, axis=1, keepdims=True))\n","    self.output = exp_values/ np.sum(exp_values,axis=1,keepdims=True)\n","    return self.output\n","\n","  def backward(self,dvalues):\n","    \"\"\" dvalues represents the gradients of the next layer. Gradients of this layer is calculated by multiplying dvalues with the jacobian matrix. \"\"\"\n","    self.dinputs = dvalues.copy()\n","    for index, (single_output,single_dvalues) in enumerate(zip(self.output,dvalues)):\n","      single_output = single_output.reshape(-1,1)\n","      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output,single_output.T)\n","      self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)\n","    return self.dinputs\n","\n","\n"],"metadata":{"id":"eH2r_gysV1JX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Cross_entropy_loss:\n","  \"\"\" This creates a cross entropy loss function. \"\"\"\n","  def __init__(self):\n","    pass\n","\n","  def forward(self,probs,labels):\n","    \"\"\" Returns the cross entropy loss. \"\"\"\n","    self.probs = probs\n","    self.labels = labels\n","\n","    m = self.labels.shape[0]\n","    self.clip = np.clip(self.probs,1e-7,1-1e-7)\n","    self.log_loss = -np.sum(self.labels*np.log(self.clip))/m\n","    return self.log_loss\n","\n","  def backward(self):\n","    \"\"\" Here the derivative of the loss is calculated with respect to the softmax output. \"\"\"\n","    m = self.labels.shape[0]\n","    self.dinputs = -(1/m)*(self.labels/self.clip)\n","    return self.dinputs"],"metadata":{"id":"fmS4SOrTWXqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MSE_loss:\n","  \"\"\" This creates a MSE loss function. \"\"\"\n","  def __init__(self):\n","    pass\n","\n","  def forward(self,probs,labels):\n","    \"\"\" Returns the MSE loss. \"\"\"\n","    self.probs = probs\n","    self.labels = labels\n","    m = self.labels.shape[0]\n","    self.loss = np.sum((self.probs-self.labels)**2)/m\n","    return self.loss\n","\n","\n","  def backward(self):\n","    \"\"\" Here the derivative of the loss is calculated with respet to the softmax output. \"\"\"\n","    m = self.labels.shape[0]\n","    self.dinputs = 2*(self.probs-self.labels)/m\n","    return self.dinputs"],"metadata":{"id":"GA1fLSooW6RW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SGD_optimizer:\n","  \"\"\" This creates a stochastic gradient descent optimizer. \"\"\"\n","  def __init__(self,learning_rate=1e-3):\n","    self.learning_rate = learning_rate\n","\n","  def step(self,layers,dvalues):\n","    \"\"\" This updates the weights and biases of the layers. \"\"\"\n","    for layer in reversed(layers):\n","      if isinstance(layer,Linear_layer):\n","        dvalues,dweights,dbias = layer.backward(dvalues)\n","        layer.weights -= self.learning_rate*dweights\n","        layer.bias -= self.learning_rate*dbias.reshape(layer.bias.shape)\n","      else:\n","        dvalues = layer.backward(dvalues)"],"metadata":{"id":"3LwLOUHrXAeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","class Model():\n","    \"\"\" This creates a model class. \"\"\"\n","    def __init__(self, layers=[]):\n","        self.layers = layers\n","\n","    def add_layer(self, layer_type, *args):\n","        \"\"\" Adds a layer to the model. \"\"\"\n","        if layer_type == 'linear':\n","            if len(args) != 2:\n","                raise ValueError('Linear_layer requires exactly two parameters')\n","            self.layers.append(Linear_layer(args[0], args[1]))\n","        elif layer_type == 'relu':\n","            self.layers.append(ReLU_activation())\n","        elif layer_type == 'sigmoid':\n","            self.layers.append(Sigmoid_activation())\n","        elif layer_type == 'tanh':\n","            self.layers.append(Tanh_activation())\n","        elif layer_type == 'softmax':\n","            self.layers.append(softmax_activation())\n","        else:\n","            raise ValueError('Invalid layer type')\n","\n","    def compile(self):\n","        \"\"\" Compiles the model. We will be using Cross-entropy loss instead of MSE loss, the reason being that MSE doesn't penalize misclassification as well as Cross-entropy. \"\"\"\n","        self.optimizer = SGD_optimizer()\n","        self.loss = Cross_entropy_loss()\n","\n","    def train(self, x_train, y_train, epochs, batch_size, x_test, y_test):\n","        \"\"\" Trains the model. Training is done in batches. \"\"\"\n","        self.x_test = x_test\n","        self.y_test = y_test\n","        for epoch in range(epochs):\n","            for batch in range(0, len(x_train), batch_size):\n","                x_batch = x_train[batch:batch+batch_size]\n","                y_batch = y_train[batch:batch+batch_size]\n","                # Forward pass\n","                activations = x_batch\n","                for layer in self.layers:\n","                    activations = layer.forward(activations)\n","\n","                # Compute loss\n","                loss = self.loss.forward(activations, y_batch)\n","\n","                # First grad\n","                dvalues = self.loss.backward()\n","\n","                # Backward pass\n","                self.optimizer.step(self.layers, dvalues)\n","\n","            # Print loss for monitoring\n","            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss}')\n","\n","    def predict(self, x_test):\n","        \"\"\" Predicts the output of the model. \"\"\"\n","        activations = x_test\n","        for layer in self.layers:\n","            activations = layer.forward(activations)\n","        return activations\n","\n","    def evaluate(self, x_test, y_test):\n","        \"\"\" Evaluates the model. \"\"\"\n","        predictions = self.predict(x_test)\n","        return self.loss.forward(predictions, y_test)\n","\n","    def save(self, filename):\n","        \"\"\" Saves the model. \"\"\"\n","        model_data = {\n","            'layers': self.layers,\n","            'optimizer': self.optimizer,\n","            'loss': self.loss\n","        }\n","        with open(filename, 'wb') as file:\n","            pickle.dump(model_data, file)\n","        print(f\"Model saved to {filename}\")\n","\n","    def load(self, filename):\n","        \"\"\" Loads the model. \"\"\"\n","        with open(filename, 'rb') as file:\n","            model_data = pickle.load(file)\n","            self.layers = model_data['layers']\n","            self.optimizer = model_data['optimizer']\n","            self.loss = model_data['loss']\n","        print(f\"Model loaded from {filename}\")\n","\n"],"metadata":{"id":"jnmVNbkHXLaQ"},"execution_count":null,"outputs":[]}]}